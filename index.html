<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Jiannan Wu</title>
    <meta name="description" content="Jiannan Wu's Homepage">
    <meta name="keywords" content="Jiannan Wu">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
    <meta name="author" content="Jiannan Wu">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <!-- <link rel="icon" type="image/png" href="images/JHU_icon.jpg"> -->
</head>

<body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:70%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Jiannan Wu (吴剑南)</name>
                                    </p>
                                    <p>
                                    I am a fourth-year (2020-now) Ph.D. student in Department of Computer Science, The University of Hong Kong, advised by <a href="http://luoping.me/">Prof. Ping Luo</a> and <a href="https://www.cs.hku.hk/people/academic-staff/wenping">Prof. Wenping Wang</a>. 
                                    Before that, I obtained my bachelor and master degree from Electrical Engineering Department, Xi'an Jiaotong University.
                                    </p>
                                    <p>
                                    My research interest lies in computer vision and deep learning. Previously, I mainly work on instance-level understanding in images and videos, including object segmentation and object tracking. 
                                    Currently, my research focuses on multimodal large language models, generalist foundation models.
<!--                                     I was fortunate to be research intern at SenseTime (Shenzhen, China), ByteDance (Beijing, China), Shanghai AI Laboratory (Shanghai, China). -->
                                    </p>
                                    <p>
                                    Please feel free to drop me an email if you are intested in my research or seek for possible collaborations.
                                    </p>					
                                    <p style="text-align:center">
                                        <a href="mailto:wjn922@connect.hku.hk">Email</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=1euA66EAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                                        <!-- <a href="">CV</a> &nbsp/&nbsp -->
                                        <a href="https://github.com/wjn922">Github</a>
                                    </p>
<!--                                </td>-->
<!--                                <td style="padding:2.5%;width:300%;max-width:300%">-->
<!--                                    <a href="images/ShilongZhang.jpg"><img style="width:110%;max-width:110%;border-radius:15%" alt="profile photo" src="images/ShilongZhang.jpg" class="hoverZoomLink"></a>-->
<!--                                </td>-->
                            </tr>
                        </tbody>
                    </table>
                    <hr>

<!--                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--                        <tbody>-->
<!--                            <tr>-->
<!--                                <td style="padding:10px;width:100%;vertical-align:middle">-->
<!--                                    <heading>News</heading>-->
<!--                                </td>-->
<!--                            </tr>-->
<!--                        </tbody>-->
<!--                    </table>-->
<!--                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
<!--                        <tr>-->
<!--                            <ul>-->
<!--                                <li> <b>[2024/4/22]</b> We propose Groma</li>-->
<!--                            </ul>-->
<!--                        </tr>-->
<!--                    </table>-->
<!--                    <hr>-->

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:10px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/vllmv2.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://arxiv.org/abs/2406.08394">
                                  <papertitle>
                                    VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks
                                  </papertitle>
                                </a>
                                <br>
                                <b>Jiannan Wu*</b>, Muyan Zhong*, Sen Xing*, Zeqiang Lai*, Zhaoyang Liu*, Zhe Chen*, Wenhai Wang*, Xizhou Zhu, Lewei Lu, Tong Lu, Ping Luo, Yu Qiao, Jifeng Dai
                                <br>
                                <em>arxiv preprint</em>, June, 2024
                                <br>
                                <a href="https://arxiv.org/abs/2406.08394">Paper</a> /
                                <a href="https://github.com/OpenGVLab/VisionLLM">Code</a> /
                                <a href="https://wjn922.github.io/visionllmv2.github.io/">Project Page</a>
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>
                    
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/groma.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://arxiv.org/abs/2404.13013">
                                  <papertitle>
                                    Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models
                                  </papertitle>
                                </a>
                                <br>
                                Chuofan Ma, Yi Jiang, <b>Jiannan Wu</b>, Zehuan Yuan, Xiaojuan Qi
                                <br>
                                <em>arxiv preprint</em>, April, 2024
                                <br>
                                <a href="https://arxiv.org/abs/2404.13013">Paper</a> /
                                <a href="https://github.com/FoundationVision/Groma">Code</a> /
                                <a href="https://groma-mllm.github.io/">Project Page</a>
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>
                    
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/internvl.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://arxiv.org/abs/2312.14238">
                                  <papertitle>
                                    InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks
                                  </papertitle>
                                </a>
                                <br>
                                Zhe Chen, <b>Jiannan Wu</b>, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, Jifeng Dai
                                <br>
                                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2024, (<b>Oral</b>)
                                <br>
                                <a href="https://arxiv.org/abs/2312.14238">Paper</a> /
                                <a href="https://github.com/OpenGVLab/InternVL">Code</a> 
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>
                    
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/visionllm.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://arxiv.org/abs/2305.11175">
                                  <papertitle>
                                    VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks
                                  </papertitle>
                                </a>
                                <br>
                                  Wenhai Wang*, Zhe Chen*, Xiaokang Chen*, <b>Jiannan Wu*</b>, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai
                                <br>
                                <em>Advances in Neural Information Processing Systems (<b>NeurIPS</b>)</em>, 2023
                                <br>
                                <a href="https://arxiv.org/abs/2305.11175">Paper</a> /
                                <a href="https://github.com/OpenGVLab/VisionLLM">Code</a>
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/simowt.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611695">
                                  <papertitle>
                                    A Simple Baseline for Open-World Tracking via Self-training
                                  </papertitle>
                                </a>
                                <br>
                                  Bingyang Wang, Tanlin Li, <b>Jiannan Wu</b>, Yi Jiang, Huchuan Lu, You He
                                <br>
                                <em>ACM International Conference on Multimedia  (<b>ACMMM</b>)</em>, 2023
                                <br>
                                <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611695">Paper</a> /
                                <a href="https://github.com/22109095/SimOWT">Code</a> 
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>

                     <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/uniref.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://arxiv.org/abs/2312.15715">
                                  <papertitle>
                                    UniRef: Segment Every Reference Object in Spatial and Temporal Spaces
                                  </papertitle>
                                </a>
                                <br>
                                  <b>Jiannan Wu</b>, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, Ping Luo
                                <br>
                                <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2023
                                <br>
                                <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Segment_Every_Reference_Object_in_Spatial_and_Temporal_Spaces_ICCV_2023_paper.html">Paper1</a> /
                                <a href="https://arxiv.org/abs/2312.15715">Paper2</a> /
                                <a href="https://github.com/FoundationVision/UniRef">Code</a> 
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/sword.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://arxiv.org/abs/2308.04206">
                                  <papertitle>
                                    Exploring Transformers for Open-world Instance Segmentation
                                  </papertitle>
                                </a>
                                <br>
                                  <b>Jiannan Wu</b>, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, Ping Luo
                                <br>
                                <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2023
                                <br>
                                <a href="https://arxiv.org/abs/2308.04206">Paper</a>
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/uninext.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://arxiv.org/abs/2303.06674">
                                  <papertitle>
                                    Universal Instance Perception as Object Discovery and Retrieval
                                  </papertitle>
                                </a>
                                <br>
                                  Bin Yan, Yi Jiang, <b>Jiannan Wu</b>, Dong Wang, Ping Luo, Zehuan Yuan, Huchuan Lu
                                <br>
                                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2023
                                <br>
                                <a href="https://arxiv.org/abs/2303.06674">Paper</a> /
                                <a href="https://github.com/MasterBin-IIAU/UNINEXT">Code</a> 
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/referformer.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://arxiv.org/abs/2201.00487">
                                  <papertitle>
                                    Language as Queries for Referring Video Object Segmentation
                                  </papertitle>
                                </a>
                                <br>
                                  <b>Jiannan Wu</b>, Yi Jiang, Peize Sun, Zehuan Yuan, Ping Luo
                                <br>
                                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2022
                                <br>
                                <a href="https://arxiv.org/abs/2201.00487">Paper</a> /
                                <a href="https://github.com/wjn922/ReferFormer">Code</a> 
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>

                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                          <tr bgcolor="#ffffff">
                            <td style="padding:20px;width:35%;vertical-align:middle">
                              <img src='images/woo.png' width="250">
                            </td>
                            <td width="75%" valign="middle">
                              <p>
                                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Watch_Only_Once_An_End-to-End_Video_Action_Detection_Framework_ICCV_2021_paper.pdf">
                                  <papertitle>
                                    Watch Only Once: An End-to-End Video Action Detection Framework
                                  </papertitle>
                                </a>
                                <br>
                                  Shoufa Chen, Peize Sun, Enze Xie, Chongjian Ge, <b>Jiannan Wu</b>, Lan Ma, Jiajun Shen, Ping Luo
                                <br>
                                <em>International Conference on Computer Vision (<b>ICCV</b>)</em>, 2021
                                <br>
                                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Chen_Watch_Only_Once_An_End-to-End_Video_Action_Detection_Framework_ICCV_2021_paper.pdf">Paper</a> /
                                <a href="https://github.com/ShoufaChen/WOO">Code</a> 
                                <br>
                              </p>
                            </td>
                          </tr>
                        </tbody>
                    </table>
                    <hr> 
                    

                    
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:10px;width:100%;vertical-align:middle">
                                    <heading>Honors</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tr>
                            <ul>
                                 <li>
                                    Hong Kong PhD Fellowship, 2020 - 2024
                                </li>
                                <li>
                                    First-class Academic Scholarship, 2018, 2019
                                </li>
                                <li>
                                    First-class Recommended Postgraduate Scholarship, 2017
                                </li>
                                <li>
                                    UHV Scholarship (top1%), 2016
                                </li>
                                <li>
                                    National Scholarship, 2014, 2015
                                </li>
                            </ul>
                        </tr>
                    </table>
                    <hr> 

                    
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:10px;width:100%;vertical-align:middle">
                                    <heading>Academic Service</heading>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tr>
                            <ul>
                                 <li>
                                    Conference Review:</br>
                            	    Conference on Computer Vision and Pattern Recognition (CVPR)</br>
                            	    International Conference on Computer Vision (ICCV)</br>
                            	    European Conference on Computer Vision (ECCV)</br>
                                </li>
                            
                                <li>
                                    Journal Review:</br>
                                    IEEE Transactions on Pattern Analysis and Machine Intelligence</br>
                                    IET Computer Vision
                                </li>
                            </ul>
                        </tr>
                    </table>
                    <hr> 

                    
                    
                    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                    <tbody>
                        <tr>
                            <td width=30% align="center">
                                <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=vORS1RJEtFHFCuxP88w0BTeURyjKm1lyNK7PyTkdmuA&cl=ffffff&w=a"></script>
                            </td>
                            <td style="padding:10px">
                                <p style="text-align:right;">Website template from <a href="https://jonbarron.info/">Jon Barron</a></p>
                            </td>
                        </tr>
                    </tbody>
                </table>
                </td>
            </tr>
        </tbody>
    </table>

    <script xml:space="preserve" language="JavaScript">
        hideallbibs();
    </script>


    <!-- Google Analytics -->
<!--    <script>-->
<!--        (function(i, s, o, g, r, a, m) {-->
<!--            i['GoogleAnalyticsObject'] = r;-->
<!--            i[r] = i[r] || function() {-->
<!--                (i[r].q = i[r].q || []).push(arguments)-->
<!--            }, i[r].l = 1 * new Date();-->
<!--            a = s.createElement(o),-->
<!--                m = s.getElementsByTagName(o)[0];-->
<!--            a.async = 1;-->
<!--            a.src = g;-->
<!--            m.parentNode.insertBefore(a, m)-->
<!--        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');-->
<!--        ga('create', 'UA-131560165-1', 'auto');-->
<!--        ga('send', 'pageview');-->
<!--    </script>-->
    <!-- End Google Analytics -->


</body>

</html>
